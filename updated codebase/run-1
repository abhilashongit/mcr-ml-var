import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

# --- 1. EFFICIENT DATA INGESTION ---
train_file = 'Crude Oil WTI- Daily 2019-2024.csv'
test_file = '2025 daily data.xlsx - Sheet1.csv'

def fast_load_and_process(path, is_train=True):
    # Optimize: Read only necessary columns if possible, but for cleaning we need all first.
    df = pd.read_csv(path)
    
    # 1. Efficiency: Fast Date Parsing
    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True if is_train else False)
    
    # Standardize Names
    rename_map = {'Vol.': 'Volume', 'Change %': 'Change_Pct'} if is_train else {'Change': 'Change_Pct'}
    df.rename(columns=rename_map, inplace=True)
    
    # 1. Efficiency: Vectorized String Operations (Faster than apply/map)
    # Handle Volume 'K' and 'M' and ',' removal
    vol_str = df['Volume'].astype(str).str.replace(',', '', regex=False)
    
    # Create multipliers: 1e6 for 'M', 1e3 for 'K', 1 for others
    multipliers = vol_str.str.extract(r'([KM])')[0].map({'K': 1e3, 'M': 1e6}).fillna(1)
    numeric_part = pd.to_numeric(vol_str.str.replace(r'[KM]', '', regex=True), errors='coerce')
    
    # 1. Efficiency: Downcast to float32 to save memory
    df['Volume'] = (numeric_part * multipliers).astype('float32')
    
    if is_train:
        df['Change_Pct'] = df['Change_Pct'].astype(str).str.replace('%', '').astype('float32') / 100
    
    # Aggregation to Weekly
    df = df.sort_values('Date').set_index('Date')
    weekly = df.resample('W-MON').agg({
        'High': 'max', 'Low': 'min', 'Price': 'last',
        'Volume': 'sum', 'Change_Pct': 'mean'
    }).dropna()
    
    weekly['Range'] = (weekly['High'] - weekly['Low']).astype('float32')
    return weekly

train_weekly = fast_load_and_process(train_file, is_train=True)
test_weekly = fast_load_and_process(test_file, is_train=False)

# --- 2. TREND CONTEXT & ADAPTIVE WEIGHTS ---
def prepare_advanced_features(df, is_train=True):
    df = df.copy()
    
    # A. Immediate Context (The original logic)
    df['Lag_Vol'] = df['Volume'].shift(1)
    df['Lag_Range'] = df['Range'].shift(1)
    
    # B. Trend Context (User Request: "Analysis of earlier weeks")
    # This helps identify if a crisis is "receding" (Avg is dropping) or "building"
    df['Rolling_Mean_4W'] = df['Range'].rolling(window=4).mean().shift(1)  # 1-month trend
    df['Rolling_Mean_8W'] = df['Range'].rolling(window=8).mean().shift(1)  # 2-month trend
    
    # Momentum: Is the range expanding or contracting compared to last month?
    df['Momentum'] = df['Lag_Range'] / (df['Rolling_Mean_4W'] + 1e-5) 

    df = df.dropna()
    
    weights = None
    if is_train:
        # C. Volatility Weighting
        vol_variance = df['Range'].rolling(window=4).std().fillna(df['Range'].std())
        vol_weights = (vol_variance / vol_variance.mean())
        
        # D. Time-Decay (User Request: "Avoid old data traces")
        # Creates a decay factor from 0.5 (oldest) to 1.0 (newest)
        # This makes recent data ~2x more important than 2019 data
        n_samples = len(df)
        time_decay = np.linspace(0.5, 1.0, n_samples)
        
        # Final Weights = Volatility (Panic factor) * Time Decay (Recency factor)
        weights = vol_weights * time_decay
        
    return df, weights

train_df, train_w = prepare_advanced_features(train_weekly, is_train=True)
test_df, _ = prepare_advanced_features(test_weekly, is_train=False)

# --- 3. XGBOOST MODEL ---
features = ['Lag_Vol', 'Lag_Range', 'Rolling_Mean_4W', 'Rolling_Mean_8W', 'Momentum']
X_train = train_df[features]
y_train = train_df['Range']

X_test = test_df[features]
y_actual = test_df['Range']

model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=2000,       # Increased slightly for more complex features
    learning_rate=0.015,     # Lower learning rate for better generalization
    max_depth=6,
    subsample=0.85,
    colsample_bytree=0.85,
    random_state=42,
    n_jobs=-1                # 1. Efficiency: Use all CPU cores
)

model.fit(X_train, y_train, sample_weight=train_w)

# --- 4. PREDICTIONS & VISUALIZATION ---
preds = model.predict(X_test)
test_df['Estimated_Range'] = preds

# Visualization
plt.figure(figsize=(14, 6))
plt.plot(test_df.index, y_actual, label='Actual Range', color='black', alpha=0.6)
plt.plot(test_df.index, preds, label='Prediction (Trend-Aware + Decayed Weights)', color='blue', linestyle='--')
plt.fill_between(test_df.index, y_actual, preds, color='blue', alpha=0.1)
plt.title('Crude Oil WTI: Prediction with Trend Context & Time Decay')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.7)
plt.show()

print(f"Model MAE: {mean_absolute_error(y_actual, preds):.4f}")
